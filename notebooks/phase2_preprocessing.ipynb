{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Data Preprocessing\n",
    "# Tiền xử lý Dữ liệu\n",
    "\n",
    "## Mục tiêu / Objectives:\n",
    "1. Load raw data from CSV\n",
    "2. Handle missing values\n",
    "3. Remove duplicates\n",
    "4. Fix data types\n",
    "5. Standardize categorical values\n",
    "6. Apply feature engineering\n",
    "7. Save processed data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from feature_engineering import engineer_all_features\n",
    "from outlier_detection import analyze_outliers, apply_log_transformation\n",
    "from data_split import create_train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data / Tải dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '../data/raw/global_disaster_response_2018_2024.csv'\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"ERROR: Data file not found at {data_path}\")\n",
    "    print(\"Please download the dataset from Kaggle and place it in data/raw/ directory\")\n",
    "    print(\"Dataset URL: https://www.kaggle.com/datasets/mubeenshehzadi/global-disaster-2018-2024\")\n",
    "else:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(f\"\\nDataset shape: {df.shape}\")\n",
    "    print(f\"Number of rows: {len(df)}\")\n",
    "    print(f\"Number of columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column information\n",
    "print(\"Column Information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"Basic Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning / Làm sạch dữ liệu\n",
    "\n",
    "### 2.1 Convert date to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime\n",
    "print(\"Converting 'date' column to datetime...\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "print(f\"Date column type: {df['date'].dtype}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"\\n✓ No missing values found!\")\n",
    "else:\n",
    "    print(f\"\\nTotal missing values: {missing_values.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values if any\n",
    "# Strategy will depend on the column and amount of missing data\n",
    "\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"Handling missing values...\")\n",
    "    \n",
    "    # For numeric columns: fill with median\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            median_val = df[col].median()\n",
    "            df[col].fillna(median_val, inplace=True)\n",
    "            print(f\"  {col}: filled with median ({median_val:.2f})\")\n",
    "    \n",
    "    # For categorical columns: fill with mode\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            mode_val = df[col].mode()[0]\n",
    "            df[col].fillna(mode_val, inplace=True)\n",
    "            print(f\"  {col}: filled with mode ({mode_val})\")\n",
    "    \n",
    "    print(\"\\n✓ Missing values handled!\")\n",
    "else:\n",
    "    print(\"✓ No missing values to handle!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(f\"Removing {duplicates} duplicate rows...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"✓ Duplicates removed! New shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"✓ No duplicates found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Fix data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify and fix data types\n",
    "print(\"Current data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Ensure numeric columns are numeric\n",
    "numeric_columns = ['severity_index', 'casualties', 'economic_loss_usd', \n",
    "                  'response_time_hours', 'aid_amount_usd', \n",
    "                  'response_efficiency_score', 'recovery_days',\n",
    "                  'latitude', 'longitude']\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "print(\"\\n✓ Data types verified and fixed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Standardize categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in categorical columns\n",
    "print(\"Unique values in categorical columns:\")\n",
    "print(f\"\\nCountries ({df['country'].nunique()}): {sorted(df['country'].unique())}\")\n",
    "print(f\"\\nDisaster Types ({df['disaster_type'].nunique()}): {sorted(df['disaster_type'].unique())}\")\n",
    "\n",
    "# Standardize (trim whitespace, fix capitalization)\n",
    "df['country'] = df['country'].str.strip()\n",
    "df['disaster_type'] = df['disaster_type'].str.strip()\n",
    "\n",
    "print(\"\\n✓ Categorical values standardized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering / Kỹ thuật đặc trưng\n",
    "\n",
    "Apply comprehensive feature engineering using our custom module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering\n",
    "print(\"Applying feature engineering...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_engineered, encoders = engineer_all_features(df, fit=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Original features: {df.shape[1]}\")\n",
    "print(f\"Engineered features: {df_engineered.shape[1]}\")\n",
    "print(f\"New features added: {df_engineered.shape[1] - df.shape[1]}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display new features\n",
    "print(\"New engineered features:\")\n",
    "new_features = [col for col in df_engineered.columns if col not in df.columns]\n",
    "for i, feature in enumerate(new_features, 1):\n",
    "    print(f\"{i:2d}. {feature}\")\n",
    "\n",
    "print(f\"\\nTotal new features: {len(new_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of engineered data\n",
    "print(\"Sample of engineered data:\")\n",
    "df_engineered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Outlier Detection / Phát hiện outliers\n",
    "\n",
    "Analyze outliers using IQR and Z-score methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze outliers\n",
    "outlier_results = analyze_outliers(df_engineered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handle Skewness / Xử lý độ lệch\n",
    "\n",
    "Apply log transformation for highly skewed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to highly skewed features\n",
    "skewed_columns = ['casualties', 'economic_loss_usd', 'aid_amount_usd']\n",
    "df_engineered = apply_log_transformation(df_engineered, skewed_columns)\n",
    "\n",
    "print(\"\\n✓ Log transformations applied!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train-Test Split / Chia dữ liệu\n",
    "\n",
    "Split data into train and test sets with stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train-test split\n",
    "train_df, test_df, encoders = create_train_test_split(\n",
    "    df_engineered,\n",
    "    target_column='disaster_type',\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    output_dir='../data/processed',\n",
    "    encoders=encoders,\n",
    "    save=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Verification / Kiểm tra cuối cùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify processed data\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n1. Original Data:\")\n",
    "print(f\"   - Shape: {df.shape}\")\n",
    "print(f\"   - Features: {df.shape[1]}\")\n",
    "\n",
    "print(f\"\\n2. Engineered Data:\")\n",
    "print(f\"   - Shape: {df_engineered.shape}\")\n",
    "print(f\"   - Features: {df_engineered.shape[1]}\")\n",
    "print(f\"   - New features: {df_engineered.shape[1] - df.shape[1]}\")\n",
    "\n",
    "print(f\"\\n3. Train Set:\")\n",
    "print(f\"   - Shape: {train_df.shape}\")\n",
    "print(f\"   - Percentage: {len(train_df)/len(df_engineered)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n4. Test Set:\")\n",
    "print(f\"   - Shape: {test_df.shape}\")\n",
    "print(f\"   - Percentage: {len(test_df)/len(df_engineered)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n5. Data Quality:\")\n",
    "print(f\"   - Missing values: {df_engineered.isnull().sum().sum()}\")\n",
    "print(f\"   - Duplicates: {df_engineered.duplicated().sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Complete Engineered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the complete engineered dataset\n",
    "output_path = '../data/processed/full_engineered_data.csv'\n",
    "df_engineered.to_csv(output_path, index=False)\n",
    "print(f\"✓ Complete engineered dataset saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Proceed to `phase2_eda.ipynb` for Exploratory Data Analysis\n",
    "2. Create visualizations and insights\n",
    "3. Move to model building in Phase 3\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
